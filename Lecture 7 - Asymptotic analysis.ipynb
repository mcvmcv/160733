{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture (Ref Logan Ch 3.1) we are going to look at some notions of convergence so we can more precisely understand when our solutions are good approximations (or not).\n",
    "\n",
    "First we look at order relations, which let us compare the behaviour of two functions as they approach a fixed value.\n",
    "\n",
    "# Order relations\n",
    "\n",
    "## *little-oh* and *big-oh*\n",
    "Let $f(\\varepsilon)$ and $g(\\varepsilon)$ be defined in some neighbourhood of $\\varepsilon = 0$. We say\n",
    "\\begin{equation}\n",
    "    f(\\varepsilon) = o(g(\\varepsilon)) \\quad \\text{as} \\quad \\varepsilon \\to 0,\n",
    "\\end{equation}\n",
    "if\n",
    "\\begin{equation}\n",
    "    \\lim_{\\varepsilon \\to 0} \\left| \\frac{f(\\varepsilon)}{g(\\varepsilon)}\\right| = 0.\n",
    "\\end{equation}\n",
    "and we say that $f$ is *little oh* of $g$, or $f \\ll g$, as this says that $f$ goes to zero faster than $g$. We say\n",
    "\\begin{equation}\n",
    "    f(\\varepsilon) = O(g(\\varepsilon)) \\quad \\text{as} \\quad \\varepsilon \\to 0,\n",
    "\\end{equation}\n",
    "if there exists a positive constant $M$ such that \n",
    "\\begin{equation}\n",
    "    \\left| f(\\varepsilon) \\right| \\le M \\left| g(\\varepsilon) \\right|\n",
    "\\end{equation}\n",
    "for all $\\varepsilon$ in a (punctured) neighourhood) of $0$. In both cases the comparison function $g$ is called a *gauge function*. The same notations apply for infinite, finite, and one-sided limits, one simply specifies what the limit is to.\n",
    "\n",
    "### Example\n",
    "Let $f(x) = x^3, g(x) = x^2$. Is $f(x) = O(g(x)$ as $x \\to 0$? Is $f(x) = o(g(x))$? Can you generalise?\n",
    "\n",
    "Let's look at big-oh first. $|x^3| = |x| \\cdot x^2 < 1\\cdot x^2$, for $|x| < 1$, so $x^3 = O(x^2)$. For positive integers $m$, $n$ with $m$ < $n$, $x^n = O(x^m)$.\n",
    "\n",
    "Now little-oh. $\\lim_{x \\to 0} \\left|\\frac{x^3}{x^2} \\right| = \\lim_{x \\to 0} |x| = 0$, so $f(x) = o(g(x)$ also.\n",
    "\n",
    "### Example\n",
    "If $f(x) = O(h(x))$ as $x \\to 0$ and $g(x) = O(h(x))$ as $x \\to 0$ then is $(f + g)(x) = O(h(x))$ as $x \\to 0$? \n",
    "\n",
    "Well, by the definition, as $x \\to 0$, $|f(x)| < M |h(x)|$ and $g(x) < N |h(x)|$ for some positive numbers $M, N$, and so $|(f + g)(x)| < (M + N)h(x)$ and so $(f + g)(x) = O(h(x))$ as $x \\to 0$.\n",
    "\n",
    "### Example 3.2\n",
    "Verify $\\varepsilon^2 \\ln \\varepsilon = o(\\varepsilon)$ as $\\varepsilon \\to 0^+$.\n",
    "\n",
    "### Example 3.3\n",
    "Verify $\\sin \\varepsilon = O(\\varepsilon)$ as $\\varepsilon \\to 0^+$.\n",
    "\n",
    "### Example: Maclaurin series\n",
    "Let $f(x)$ have a convergent Maclaurin series $f(x) = a_0 + a_1 x + a_2 x^2 + \\cdots$ in some neighbourhood of $0$. Show that the error in the $k$-th order approximation $p_k(x) = a_0 + a_1 x + \\cdots + a_k x^k$, is $f(x) - p_k(x) = O(x^{k+1})$.\n",
    "\n",
    "We know that $f(x) = p_k(x) + \\frac{f^{k+1}(\\xi)}{(k+1)!} x^{k + 1}$ for some $0 < \\xi < x$, and we know that the derivatives are bounded so $\\left|f(x) - p_k(x)\\right| < M x^{k+1}$ for some positive constant $M$, i.e. $|f(x) - p_k(x)| = O(x^{k + 1})$.\n",
    "\n",
    "## Uniform convergence\n",
    "Let $h(t, \\epsilon)$ be a function defined for $\\varepsilon$ in a neighbourhood of $\\varepsilon = 0$, and for $t$ in some finite or infinite interval $I$. We say\n",
    "\\begin{equation}\n",
    "    \\lim_{\\varepsilon \\to 0} h(t, \\varepsilon) = 0 \\quad \\text{uniformly on } I,\n",
    "\\end{equation}\n",
    "if the convergence to zero is at the same rate for each $t \\in I$; that is if for any positive number $\\eta$ there can be chosen a positive number $\\varepsilon_0$, independent of $t$, such that $|h(t, \\varepsilon| < \\eta$ for all $t \\in I$, whenever $|\\varepsilon| < \\varepsilon_0$. Or, if $h(t, \\varepsilon)$ can be made arbitrarily small *over the entire interval* $I$ by choosing $\\varepsilon$ small enough. If it is only the case that $\\lim_{\\varepsilon \\to 0} h(t_0, \\varepsilon) = 0$ for each fixed $t_0 \\in I$, then the convergence is *pointwise* on $I$. \n",
    "* to prove $\\lim_{\\varepsilon \\to 0} h(t, \\varepsilon) = 0$ uniformly on $I$, it is sufficient to find a function $H(\\varepsilon)$ such that $|h(t, \\varepsilon)| \\le H(\\varepsilon)$ holds for all $t \\in I$, with $H(\\varepsilon) \\to 0$ as $\\varepsilon \\to 0$.\n",
    "* to prove convergence is not uniform, we seek a $\\bar{t} \\in I$ such that $|h(\\bar{t}, \\varepsilon)| \\ge \\eta$ for some positive $\\eta$, regardless of how small $\\varepsilon$ is.\n",
    "\n",
    "We can talk about little-oh and big-oh for these functions too: We write\n",
    "\\begin{equation}\n",
    "    f(t, \\varepsilon) = o(g(t, \\varepsilon)) \\quad \\text{as} \\quad \\varepsilon \\to 0,\n",
    "\\end{equation}\n",
    "if\n",
    "\\begin{equation}\n",
    "    \\lim_{\\varepsilon \\to 0} \\left| \\frac{f(t, \\varepsilon)}{g(t, \\varepsilon}\\right| = 0\n",
    "\\end{equation}\n",
    "pointwise on $I$. If the limit is uniform, we can append \"uniformly on $I$\".\n",
    "\n",
    "Similarly for big-oh.\n",
    "\n",
    "### Asymptotic approximations\n",
    "We say that a function $y_a(t, \\varepsilon)$ is a *uniformly valid asymptotic approximation* to a function $y(t, \\varepsilon)$ on an interval $I$ as $\\varepsilon \\to 0$ if the error $E(t, \\varepsilon) \\equiv y(t, \\varepsilon) - y_a(t, \\varepsilon)$ converges to zero as $\\varepsilon \\to 0$ uniformly for $t \\in I$. We'll often say that $E$ is big-oh or little-oh of something to express how quickly.\n",
    "\n",
    "### Example 3.4\n",
    "Let \n",
    "\\begin{equation}\n",
    "   y(t, \\varepsilon) = e^{-t\\varepsilon}, \\quad t > 0, \\quad \\varepsilon \\ll 1.\n",
    "\\end{equation}\n",
    "Let's see how well the first three terms of its Taylor expansion do as an approximation\n",
    "\\begin{equation}\n",
    "    y_a(t, \\varepsilon) = 1 - t\\varepsilon + \\frac{1}{2}t^2 \\varepsilon^2.\n",
    "\\end{equation}\n",
    "The error is then\n",
    "\\begin{equation}\n",
    "   E(t, \\varepsilon) = e^{-t\\varepsilon} - 1 + t\\varepsilon - \\frac{1}{2}t^2 \\varepsilon^2 = -\\frac{1}{3}t^3\\varepsilon^3 + \\cdots\n",
    "\\end{equation}\n",
    "* For fixed $t$ the error can be made as small as we like - convergence is pointwise\n",
    "* If $t = \\frac{1}{\\varepsilon}$, $E(\\frac{1}{\\varepsilon}, \\varepsilon) = e^{-1} - \\frac{1}{2}$, which is not small. So the convergence is not uniform.\n",
    "\n",
    "## Usage with differential equations\n",
    "When we are studying the approximate solution of differential equations, we don't usually have an exact solution to compare with. So we have to make the best of what we do have, which is the equation itself. If our differential equation is\n",
    "\\begin{equation}\n",
    "    F(t, y, y', y'', \\varepsilon) = 0, \\quad t \\in I.\n",
    "\\end{equation}\n",
    "Then we say an approximation $y_a(t, \\varepsilon)$ satisfies the differential equation uniformly for $t \\in I$ as $\\varepsilon \\to 0$ if\n",
    "\\begin{equation}\n",
    "    r(t, \\varepsilon) \\equiv F(t, y_a(t, \\varepsilon), \\dot{y}_a(t, \\varepsilon), \\ddot{y}_a(t, \\varepsilon), \\varepsilon) \\to 0\n",
    "\\end{equation}\n",
    "uniformly on $I$ as $\\varepsilon \\to 0$. This quantity is called the *residual*, and is a measure of how well the approximation satisfies the equation.\n",
    "\n",
    "### Example 3.6\n",
    "\\begin{align}\n",
    "    \\ddot{y} + \\dot{y}^2 + \\varepsilon y &= 0, & t &> 0, & 0 < \\varepsilon \\ll 1,\\\\\n",
    "    y(0) &= 0, & \\dot{y}(0) &= 1.\n",
    "\\end{align}\n",
    "Let's just look at the leading order equation:\n",
    "\\begin{align}\n",
    "    \\ddot{y}_0 + \\dot{y}_0^2 &=0, & t &> 0,\n",
    "    y_0(0) &= 0, & \\dot{y}_0(0) = 1\n",
    "\\end{align}\n",
    "The solution to this is $y_0(t) = \\ln(t + 1)$, and hence\n",
    "\\begin{equation}\n",
    "    r(t, \\varepsilon) \\equiv \\ddot{y}_0 + \\dot{y}_0^2 + \\varepsilon y_0 = \\varepsilon \\ln (t + 1)\n",
    "\\end{equation}\n",
    "So $r(t, \\varepsilon) = O(\\varepsilon)$ as $\\varepsilon \\to 0$, but not uniformly on $[0, \\infty)$. If our interval was finite, however, $[0, T]$, we have $|\\varepsilon \\ln(t + 1)| \\le \\varepsilon \\ln(T + 1)$, and so $r(t, \\varepsilon) = O(\\varepsilon)$ as $\\varepsilon \\to 0$ uniformly on $[0, T]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
